import google.generativeai as genai
import time
import json
from PIL import Image
import streamlit as st
import hashlib
from datetime import datetime, timedelta

# --- C·∫§U H√åNH MODEL ---
MODEL_NAME = "gemini-2.5-flash"

# --- RATE LIMITER ---
if 'last_api_call' not in st.session_state:
    st.session_state.last_api_call = None

@st.cache_resource
def get_model():
    return genai.GenerativeModel(MODEL_NAME)

def enforce_minimum_delay(min_seconds=4):
    """B·∫ÆT BU·ªòC delay 4s gi·ªØa m·ªói API call"""
    if st.session_state.last_api_call:
        elapsed = (datetime.now() - st.session_state.last_api_call).total_seconds()
        if elapsed < min_seconds:
            wait_time = min_seconds - elapsed
            with st.spinner(f"‚è≥ Rate limiting... {wait_time:.1f}s"):
                time.sleep(wait_time)
    st.session_state.last_api_call = datetime.now()

def safe_api_call(func, *args, **kwargs):
    """Retry v·ªõi exponential backoff"""
    backoff_times = [5, 10, 20, 40, 60]
    
    for attempt, wait_time in enumerate(backoff_times):
        try:
            enforce_minimum_delay(4)
            result = func(*args, **kwargs)
            return result
        except Exception as e:
            error_msg = str(e)
            if any(x in error_msg for x in ["429", "ResourceExhausted", "503", "quota"]):
                if attempt < len(backoff_times) - 1:
                    st.warning(f"‚è≥ Server busy. Waiting {wait_time}s... ({attempt+1}/{len(backoff_times)})")
                    time.sleep(wait_time)
                    continue
                else:
                    st.error("üö´ Server qu√° t·∫£i. Vui l√≤ng ƒë·ª£i 2-3 ph√∫t r·ªìi th·ª≠ l·∫°i.")
                    return None
            else:
                st.error(f"‚ùå L·ªói: {error_msg[:150]}")
                return None
    return None

# --- C√ÅC H√ÄM API v·ªõi @st.cache_data ---

@st.cache_data(ttl=7200, show_spinner=False)
def get_ai_recommendations(age, interests, mood, style, content_type):
    """AI Recommendations - Cache 2 gi·ªù"""
    model = get_model()
    prompt = f"""
Act as an expert OTAKU. Recommend 5 {content_type} series.
User Info: {age} years old.
Interests: {interests}
Current Mood: {mood}
Preferred Style: {style}

IMPORTANT: Return ONLY valid JSON array. No markdown, no backticks.
Format: [{{"title": "Name", "genre": "Genre", "reason": "Why this fits"}}]
"""
    
    def _call():
        response = model.generate_content(prompt)
        text = response.text.strip().replace("```json", "").replace("```", "").strip()
        return json.loads(text)
    
    result = safe_api_call(_call)
    return result if result else []

@st.cache_data(ttl=86400, show_spinner=False)
def ai_vision_detect_cached(image_bytes):
    """Vision Detection - Cache 24 gi·ªù theo image bytes"""
    model = get_model()
    from io import BytesIO
    img = Image.open(BytesIO(image_bytes))
    
    prompt = """Look at this anime character image.
Return ONLY the character's full name (e.g. "Naruto Uzumaki").
If cannot identify, return exactly: "Unknown"
No explanation, just the name."""
    
    def _call():
        response = model.generate_content([prompt, img])
        return response.text.strip()
    
    result = safe_api_call(_call)
    return result if result else "Unknown"

def ai_vision_detect(image_file):
    """Wrapper cho vision detection"""
    image_file.seek(0)
    image_bytes = image_file.read()
    image_file.seek(0)
    return ai_vision_detect_cached(image_bytes)

@st.cache_data(ttl=86400, show_spinner=False)
def generate_ai_profile_text(char_id, char_name, char_about):
    """
    Generate AI Profile - Cache 24 gi·ªù theo char_id
    ƒê√ÇY L√Ä H√ÄM CH√çNH ƒê·ªÇ CACHE
    """
    model = get_model()
    
    if char_about and len(char_about) > 2000:
        char_about = char_about[:2000] + "..."

    prompt = f"""You are an expert Anime Otaku. Write an engaging character profile in ENGLISH.

Character Name: {char_name}
Biography: {char_about}

Requirements:
- Catchy title with emojis (üåüüî•‚ú®)
- Fun, enthusiastic tone
- Analyze personality and powers
- Keep under 200 words
- Make it engaging!"""
    
    def _call():
        response = model.generate_content(prompt)
        return response.text.strip()
    
    result = safe_api_call(_call)
    return result if result else "‚ö†Ô∏è Could not generate profile. Please try again later."

def generate_ai_stream(info):
    """
    Wrapper cho stream - G·ªåI H√ÄM CACHED
    Tr·∫£ v·ªÅ list chunks ƒë·ªÉ t∆∞∆°ng th√≠ch v·ªõi code c≈©
    """
    char_id = info.get('mal_id')
    char_name = info.get('name', 'N/A')
    char_about = info.get('about', 'N/A')
    
    # G·ªåI H√ÄM CACHED - N·∫øu c√≥ cache th√¨ return ngay l·∫≠p t·ª©c
    full_text = generate_ai_profile_text(char_id, char_name, char_about)
    
    # Fake stream ƒë·ªÉ UX m∆∞·ª£t
    class TextChunk:
        def __init__(self, text):
            self.text = text
    
    return [TextChunk(full_text)]
